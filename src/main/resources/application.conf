akka {
  // loglevel = DEBUG
  actor {
    provider = cluster
    warn-about-java-serializer-usage = false
  }

  remote.artery {
    enabled = on
    transport = tcp
    canonical.port = 0
    canonical.hostname = 192.168.43.183
  }

  cluster {
    seed-nodes = [
      "akka://akkaMessages@192.168.43.183:2551"
      // "akka://akkaMessages@192.168.43.247:2551"
      ]

    auto-down-unreachable-after = 20s
    sharding.state-store-mode = ddata
    sharding.remember-entities = off
    sharding.waiting-for-state-timeout = 10000
    min-nr-of-members = 1

//    distributed-data {
//      # Actor name of the Replicator actor, /system/ddataReplicator
//      name = ddataReplicator
//
//      # Replicas are running on members tagged with this role.
//      # All members are used if undefined or empty.
//      role = ""
//
//      # How often the Replicator should send out gossip information
//      gossip-interval = 2 s
//
//      # How often the subscribers will be notified of changes, if any
//      notify-subscribers-interval = 500 ms
//
//      # Maximum number of entries to transfer in one gossip message when synchronizing
//      # the replicas. Next chunk will be transferred in next round of gossip.
//      max-delta-elements = 500
//
//      # The id of the dispatcher to use for Replicator actors.
//      # If specified you need to define the settings of the actual dispatcher.
//      use-dispatcher = "akka.actor.internal-dispatcher"
//
//      # How often the Replicator checks for pruning of data associated with
//      # removed cluster nodes. If this is set to 'off' the pruning feature will
//      # be completely disabled.
//      pruning-interval = 120 s
//
//      # How long time it takes to spread the data to all other replica nodes.
//      # This is used when initiating and completing the pruning process of data associated
//      # with removed cluster nodes. The time measurement is stopped when any replica is
//      # unreachable, but it's still recommended to configure this with certain margin.
//      # It should be in the magnitude of minutes even though typical dissemination time
//      # is shorter (grows logarithmic with number of nodes). There is no advantage of
//      # setting this too low. Setting it to large value will delay the pruning process.
//      max-pruning-dissemination = 300 s
//
//      # The markers of that pruning has been performed for a removed node are kept for this
//      # time and thereafter removed. If and old data entry that was never pruned is somehow
//      # injected and merged with existing data after this time the value will not be correct.
//      # This would be possible (although unlikely) in the case of a long network partition.
//      # It should be in the magnitude of hours. For durable data it is configured by
//      # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
//      pruning-marker-time-to-live = 6 h
//
//      # Serialized Write and Read messages are cached when they are sent to
//      # several nodes. If no further activity they are removed from the cache
//      # after this duration.
//      serializer-cache-time-to-live = 10s
//
//      # Settings for delta-CRDT
//      delta-crdt {
//        # enable or disable delta-CRDT replication
//        enabled = on
//
//        # Some complex deltas grow in size for each update and above this
//        # threshold such deltas are discarded and sent as full state instead.
//        # This is number of elements or similar size hint, not size in bytes.
//        max-delta-size = 50
//      }
//    }
  }
    coordinated-shutdown.exit-jvm = on
}